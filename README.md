# ğŸ¢ Qwen Code - On-Premise AI Coding Assistant

![Qwen Code Screenshot](./docs/assets/one-screenshot.PNG)

**Complete Offline AI Development Tool for Enterprises and Government**

Qwen Code is a command-line AI workflow tool that operates in air-gapped environments. It enables complete code analysis, generation, and refactoring using only internal LLM servers without external internet access.

## ğŸ¯ On-Premise Specialized Features

### âœ¨ Core Features

- **ğŸ”’ Fully Offline**: No external internet connection required
- **ğŸ—ï¸ Internal LLM Integration**: OpenAI API compatible internal server support
- **ğŸŒ Internal Web Search**: Enterprise internal documentation/wiki search
- **ğŸ›¡ï¸ SSL Bypass**: Secure communication with internal servers without certificates
- **ğŸŒ Multi-language Support**: Natural Korean responses with English technical docs
- **ğŸ¤– SuperClaude Integration**: Advanced AI workflow with 11 expert personas

### ğŸ’¼ Enterprise Use Cases

- **Financial Institutions**: Security-critical code development environments
- **Government Agencies**: AI assistant in air-gapped networks
- **Large Enterprises**: Coding tools compliant with internal policies
- **Research Labs**: Code analysis for confidential projects

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### Requirements
- **Node.js**: 20.x or higher (LTS recommended)
- **Git**: Latest version
- **Terminal**: PowerShell (Windows), bash/zsh (Linux/macOS)

### Step 1: Project Installation

```bash
# 1. Clone repository
git clone https://github.com/your-org/qwen-code-on-premise.git
cd qwen-code-on-premise

# 2. Install dependencies
npm install

# 3. Global installation
npm install -g .

# 4. Verify installation
qwen-one --version
```

### Step 2: Internal Server Connection

**Method 1: Using .env file (Recommended)**
```bash
# Copy and edit the example file
cp .env.example .env
# Edit .env with your settings

# Or use interactive setup
# Windows PowerShell:
.\scripts\env-setup.ps1 -Interactive

# Linux/macOS:
./scripts/env-setup.sh --interactive
```

**Method 2: Environment Variables**
```bash
# PowerShell
$env:OPENAI_BASE_URL = "http://your-internal-llm:8080/v1"
$env:OPENAI_API_KEY = "internal-api-key"
$env:OPENAI_MODEL = "your-internal-model"
$env:ON_PREMISE_MODE = "true"
$env:NODE_TLS_REJECT_UNAUTHORIZED = "0"

# bash/zsh
export OPENAI_BASE_URL="http://your-internal-llm:8080/v1"
export OPENAI_API_KEY="internal-api-key"
export OPENAI_MODEL="your-internal-model"
export ON_PREMISE_MODE="true"
export NODE_TLS_REJECT_UNAUTHORIZED="0"
```

### Step 3: SuperClaude Commands Usage

```bash
# Basic usage
qwen-one
# Then type: Create a REST API server in Python

# SuperClaude expert mode (automatic activation)
qwen-one
# Then type: Analyze this project from security and performance perspectives

# Project analysis
qwen-one
# Then type: Analyze the entire codebase architecture

# Code improvement
qwen-one
# Then type: Improve code quality and remove technical debt
```

### Step 4: Development Environment Setup (Optional)

```bash
# Run in development mode
npm start

# Build and test
npm run build
npm test

# Full validation
npm run preflight
```

## ğŸ“š Complete Documentation

### ğŸ“– **[Product Requirements and Development Plan](docs/custom-on-premise/1_product_and_development_plan.md)**

- Original product requirements and detailed development plan with SuperClaude framework integration

### ğŸ“– **[User Guide and Development Philosophy](docs/custom-on-premise/2_user_guide_and_philosophy.md)**

- Comprehensive installation guide, SuperClaude expert system usage, and development philosophy

### ğŸ“– **[Model Integration and Evaluation](docs/custom-on-premise/3_model_integration_and_evaluation.md)**

- Integration tests and performance evaluation with SuperClaude framework enhancements

### ğŸ“– **[Qwen Model Details](README.QWEN.md)**

- Detailed Qwen model information and Korean language features

## ğŸ› ï¸ Real Usage Examples

### Code Development

```bash
# High-quality code generation with expert guidance
qwen-one
> Create an Express.js API with JWT token authentication
# â†’ Backend expert automatically activated

# Code refactoring
qwen-one
> Refactor this function to be more readable and maintainable
# â†’ Refactorer expert automatically activated

# Bug analysis
qwen-one
> Analyze this error log and provide solutions
# â†’ Analyzer expert automatically activated
```

### Internal Documentation

```bash
# Internal wiki search
qwen-one
> Find coding style guidelines in company development standards
# â†’ Internal web search with document analysis

# API documentation search
qwen-one
> Show user authentication methods from internal API docs
# â†’ Automatic URL selection and content extraction

# Technical document analysis
qwen-one
> Summarize the new architecture document
# â†’ Architect expert with document analysis
```

### SuperClaude Advanced Features

```bash
# Expert persona utilization (automatic activation)
qwen-one
> Analyze the system architecture and identify scalability issues
# â†’ Architect expert automatically activated

qwen-one  
> Review this code for security vulnerabilities
# â†’ Security expert automatically activated

# Advanced project analysis
qwen-one
> Perform comprehensive system analysis with performance optimization
# â†’ Multiple experts activated: Architect + Performance + Analyzer

# Iterative improvement
qwen-one
> Improve code quality systematically across the entire project
# â†’ Refactorer expert with structured workflow management
```

## ğŸ”’ Security and Compatibility

### âœ… Verified Environments

- **Windows 10/11**: PowerShell and Command Prompt
- **Linux/Unix**: bash, zsh shell environments  
- **macOS**: Terminal.app and iTerm2
- **Air-gapped Networks**: Complete offline operation verified

### ğŸ›¡ï¸ Security Features

- **SSL Certificate Bypass**: Secure communication with internal development servers
- **API Key Protection**: Authentication information used only within internal networks
- **Minimal Logging**: Prevention of sensitive information logging
- **Completely Local**: No data transmission to external servers

### ğŸ”— Compatible LLM Servers

```bash
# All OpenAI API compatible servers supported
export OPENAI_MODEL="company-llama"     # âœ… Llama family
export OPENAI_MODEL="internal-claude"   # âœ… Claude family
export OPENAI_MODEL="our-gpt"          # âœ… GPT family
export OPENAI_MODEL="qwen3-235b"        # âœ… Qwen family
export OPENAI_MODEL="korean-ai"         # âœ… Korean model names supported
```

## ğŸ“Š Performance Benchmarks

### OpenRouter Qwen Test Results with SuperClaude

- **Code Implementation**: â­â­â­â­â­ (95/100)
- **Korean Language Fluency**: â­â­â­â­â­ (95/100)
- **Logical Reasoning**: â­â­â­â­â­ (95/100)
- **Cultural Understanding**: â­â­â­â­â­ (90/100)
- **Tool Utilization**: â­â­â­â­â­ (95/100)
- **Expert System Integration**: â­â­â­â­â­ (98/100)

Detailed analysis: [Model Integration and Evaluation](docs/custom-on-premise/3_model_integration_and_evaluation.md)

## ğŸ† Competitive Advantages

### vs Existing Coding Assistants

| Feature | Qwen Code | GitHub Copilot | Claude Code |
| ------- | --------- | -------------- | ----------- |
| Offline Operation | âœ… Complete Support | âŒ Online Required | âŒ Online Required |
| Internal Server Integration | âœ… Perfect Support | âŒ Not Possible | âŒ Limited |
| Multi-language Support | âœ… Natural Korean + English | ğŸ”¶ Basic Support | âœ… Excellent |
| Enterprise Security | âœ… Complete Isolation | âŒ External Transmission | âŒ External Transmission |
| Internal Document Search | âœ… Dedicated Feature | âŒ Not Possible | âŒ Not Possible |
| Expert System | âœ… 11 Specialized Personas | âŒ Generic Assistant | âŒ Single Persona |
| Workflow Management | âœ… Structured Task Tracking | âŒ Simple Chat | ğŸ”¶ Basic Tasks |

### Real Deployment Cases

- **Financial Institution A**: Core banking system code analysis
- **Government Agency B**: AI assistant operation in air-gapped environment
- **Large Enterprise C**: Internal API documentation automatic analysis system

## ğŸ’¡ Advanced Usage Tips

### Internal Web Search Optimization

```json
// internal-web-config.json configuration example
{
  "sites": [
    {
      "name": "Company API Documentation",
      "baseUrl": "http://internal-docs.company.com",
      "searchEndpoint": "/search",
      "priority": 1
    },
    {
      "name": "Development Wiki",
      "baseUrl": "http://wiki.company.com",
      "searchEndpoint": "/api/search",
      "priority": 2
    }
  ]
}
```

### Team Configuration Sharing

```bash
# Create team common configuration file
cat > team-settings.sh << 'EOF'
export OPENAI_BASE_URL="http://our-llm:8080/v1"
export OPENAI_API_KEY="team-shared-key"
export OPENAI_MODEL="our-fine-tuned-model"
export ON_PREMISE_MODE="true"
export NODE_TLS_REJECT_UNAUTHORIZED="0"
EOF

# Team members use common settings
source team-settings.sh
```

## ğŸ†˜ Troubleshooting

### Frequently Asked Questions

**Q: npm errors during installation**

```bash
# Run PowerShell as administrator on Windows
npm install -g @qwen-code/qwen-code --force
```

**Q: Cannot connect to internal server**

```bash
# Resolve SSL certificate issues
export NODE_TLS_REJECT_UNAUTHORIZED="0"
# Or test server connection with curl
curl -k http://your-internal-llm:8080/v1/models
```

**Q: Korean responses are unnatural**

```bash
# Launch qwen-one and request in Korean
qwen-one
> íŒŒì´ì¬ í´ë˜ìŠ¤ ìƒì†ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”
```

Detailed troubleshooting: [User Guide](docs/custom-on-premise/2_user_guide_and_philosophy.md)

## ğŸ”„ Updates and Support

### Version Management

```bash
# Check current version
qwen-one --version

# Update to latest version
npm update -g @qwen-code/qwen-code

# Install specific version
npm install -g @qwen-code/qwen-code@1.2.3
```

### Community Support

- **GitHub Issues**: Bug reports and feature requests
- **Technical Documentation**: Detailed guides for all configurations and usage
- **Example Collection**: Real use cases and scripts

## ğŸ“‹ License and Contributing

### Open Source License

This project is based on [Google Gemini CLI](https://github.com/google-gemini/gemini-cli) and follows the same license.

### How to Contribute

We welcome improvements and bug reports for on-premise environments:

- Refer to [Contributing Guide](./CONTRIBUTING.md)
- Submit [Issue Reports](https://github.com/ringo-hik/qwen-code/issues)
- Share internal environment test results

---

## ğŸš€ Get Started Now

```bash
# Complete installation in 1 minute
git clone https://github.com/your-org/qwen-code-on-premise.git
cd qwen-code-on-premise
npm install && npm install -g .

# Internal server configuration
export OPENAI_BASE_URL="http://your-internal-llm:8080/v1"
export ON_PREMISE_MODE="true"

# Start using immediately
qwen-one
# Then type: Hello! Please help me with coding.
```

**Complete offline AI coding assistant with SuperClaude expert system - Experience it now! ğŸ‰**

---

**Original Documentation**: [README.QWEN.md](README.QWEN.md) - Complete Qwen Code documentation  
**Development Team**: On-premise specialization based on Claude Code SuperClaude  
**Last Updated**: 2025-01-27
